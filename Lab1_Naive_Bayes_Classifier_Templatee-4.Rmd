---
output:
  html_document: default
  pdf_document: default
---
---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work breakdown

-   *Pavlo Ruzhylo*: Data preprocessing, model fitting, measurements of effectiveness of a model.
-   *Ivan Polianskyy*: Data visualization
-   *Maksym Matseliukh*: Conclusions

## Introduction

During the first three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the **Bayes
theorem**.

**Naive Bayes Classifier** is a simple algorithm, which is based on
**Bayes theorem** and used for solving classification problems.
**Classification problem** is a problem in which an observation has to
be classified in one of the $n$ classes based on its similarity with
observations in each class.

It is a **probabilistic classifier**, which means it predicts based on
the probability of an observation belonging to each class. To compute
it, this algorithm uses **Bayes' formula,** that you probably already
came across in **Lesson 3:**
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong **independence** assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given observation
(*For example, if an observation is presented as a sentence, then each
word can be a feature*). Thus,
$\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now can be calculated
as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated as corresponding
relative frequencies using available data\

**See [*this
link*](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)
for more detailed explanations & examples :) Also you can watch [*this
video*](https://youtu.be/O2L2Uv9pdDA?si=-ohkHVDuu3sLLGMq) for more
examples!**

## Data description

There are 5 datasets uploaded on the cms (data.zip)

Our dataset:
-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one is used to find the probabilities of the corresponding classes
and the second one is used to test your classifier afterwards. Note that
it is crucial to randomly split your data into training and testing
parts to test the classifierʼs possibilities on the unseen data.

```{r}
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(wordcloud)
library(RColorBrewer)
```

## Outline of the work

1.  **Data pre-processing** (includes removing punctuation marks and
    stop words, representing each message as a bag-of-words)
2.  **Data visualization** (it's time to plot your data!)
3.  **Classifier implementation** (using the training set, calculate all
    the conditional probabilities in formula (1))
4.  **Measurements of effectiveness of your classifier** (use the
    results from the previous step to predict classes for messages in
    the testing set and measure the accuracy, precision and recall, F1
    score metric etc)
5.  **Conclusions**

*!! do not forget to submit both the (compiled) Rmd source file and the
.html output !!*

## Data pre-processing

-   Read the *.csv* data files.
-   Сlear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.


```{r}
test_path <- "data/0-authors/test.csv"
train_path <- "data/0-authors/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]
# splitted_stop_words
# head(splitted_stop_words)
```
```{r}
train <-  read.csv(file = train_path, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE, quote="\"")
```

```{r}
# note the power functional features of R bring us! 
tidy_text <- unnest_tokens(train[, ], "splitted", 'text', token="words") %>%
    filter(!splitted %in% splitted_stop_words)

X_test <- test$text
y_test <- test$author
```

```{r}
#Converting each message into bag of words:
vocabulary <- unique(tidy_text$splitted)
authors <- unique(tidy_text$author)

bow <- tidy_text %>% count(id, splitted)
author_labels <- tidy_text %>%
  select(id, author) %>%
  distinct()
bow <- bow %>%
  pivot_wider(
    names_from = splitted,
    values_from = n,
    values_fill = list(n = 0),
    names_repair="unique"
  )


```

## Data visualization

```{r}

word_freq <- tidy_text %>%
  group_by(author, splitted) %>%
  summarise(freq = n(), .groups = "drop") %>%
  arrange(author, desc(freq))


overall_word_freq <- tidy_text %>%
  count(splitted, sort = TRUE) %>%
  head(15) #<<<<<-------???????????


ggplot(overall_word_freq, aes(x = reorder(splitted, n), y = n)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  coord_flip() +
  labs(title = "Top 15 Most Common Words in Cleaned Training Data",
       x = "Words", y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))
```

```{r}

top_words_per_author <- word_freq %>%
  group_by(author) %>%
  slice_head(n = 10) %>%
  ungroup()

ggplot(top_words_per_author, aes(x = reorder(splitted, freq), y = freq, fill = author)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~author, scales = "free_y") +
  labs(title = "Top 10 Words by Author",
       x = "Words", y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "none")
```

```{r}

if (require(wordcloud, quietly = TRUE) && require(RColorBrewer, quietly = TRUE)) {
  par(mfrow = c(1, 3))
  for (author_name in authors) {
    author_words <- word_freq %>%
      filter(author == author_name) %>%
      head(30)
    
    wordcloud(words = author_words$splitted, 
              freq = author_words$freq,
              max.words = 30,
              random.order = FALSE,
              colors = brewer.pal(8, "Dark2"),
              main = paste("Word Cloud:", author_name))
  }
  par(mfrow = c(1, 1))
} else {
  cat("Word clouds skipped - wordcloud or RColorBrewer not available\n")
}
```
## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
       fields = list(
         class_priors = "table",
         #Word probabilities -> probabilities of words given class.
         #Structure:
         #HP Lovecraft
         #hello   free  money    now 
        #  0.25   0.25   0.40   0.10
         word_probabilities = "list",
        
        #A vector of all words available in the training data
         vocabulary = "character",
        
        #A number of all words in training data used to find P(feature)
        total_words_in_X = "numeric",
        
        computed_score = "data.frame"
        
       ),
       methods = list(
                    # prepare your training data as X - bag of words for each of your
                    # messages and corresponding label for the message encoded as 0 or 1 
                    # (binary classification task)
                    fit = function(X, y)
                    {
                      X <- X[order(X$id...1), ]
                      y <- y[order(y$id), ]
                      
                      stopifnot(all(X$id...1 == y$id))
                      
                      vocabulary <<- setdiff(colnames(X), "id...1")
                      class_counts <- table(y$author)
                      total_words_in_X <<- sum(colSums(X[, 2:ncol(X)]))

   
                      class_priors <<- class_counts / sum(class_counts)
                      word_probabilities <<- list()
                        for (class in names(class_counts)) {
                          # subset X where y == class
                          X_class <- X[y$author == class, , drop = FALSE]
                          # sum counts of each word in that class
                          word_counts <- colSums(X_class[, 2:ncol(X_class)])
                          
                          # add +1 to each count to avoid multiplying by 0
                          word_counts <- word_counts + 1
                          total_count <- sum(word_counts)
                          

                          probs <- word_counts / total_count
                          
                          word_probabilities[[class]] <<- probs
                    }
                      
                      
                         
                    },
                    
                    # return prediction for a single message 
                      predict = function(message) {
                        message_df <- data.frame(text = message, stringsAsFactors = FALSE)
                        tidy_message <- message_df %>%
                          unnest_tokens(splitted, text, token = "words") %>%
                          filter(!(splitted %in% splitted_stop_words)) %>%
                          filter((splitted %in% vocabulary))
                        probs_per_class <- c()
                        
                        for (class in names(class_priors)) {
                          p_class <- class_priors[class]
                          p_observation_given_class <- 1
                          
                          for (word in tidy_message$splitted) {
                            p_w_given_class <- word_probabilities[[class]][word]
                            p_observation_given_class <- p_observation_given_class * p_w_given_class
                          }
                          
                          probs_per_class[class] <- (p_class * (p_observation_given_class))
                        }
                        
                        predicted_class <- names(which.max(probs_per_class))
                        return(predicted_class)

                    }
                  ,
                                      

                      score = function(X_test, y_test)
                      {
                            confusion <- matrix(0, nrow = 3, ncol = 3)
                            rownames(confusion) <- c("Edgar Alan Poe", "HP Lovecraft", "Mary Wollstonecraft Shelley ")
                            colnames(confusion) <- c("Edgar Alan Poe", "HP Lovecraft", "Mary Wollstonecraft Shelley ")
                            failures <- vector("list", length(X_test))
                            count <- 0
                            for (i in seq_along(X_test)) {
                                pred <- predict(X_test[i])
                                true <- y_test[i]
                                confusion[true, pred] <- confusion[true, pred] + 1
                                if (pred != true) {
                                    count <- count + 1
                                    failures[[count]] <- X_test[i]
                                }
                            }
                            
                            confusion
                            
                            scores <- data.frame(
                              Class=rownames(confusion),
                              TP=0, FP=0, FN=0,
                              Precision=0, Recall=0, F1=0)
                            total <- sum(confusion)
                            
                            for (i in 1:3) {
                              tp <- confusion[i, i]
                              fp <- sum(confusion[-i, i])
                              fn <- sum(confusion[i, -i])
                              precision <- tp / (tp + fp)
                              recall <- tp / (tp + fn)
                              f1 <- (2*precision*recall /(precision+recall))
                              scores[i, 2:ncol(scores)] <- c(tp, fp, fn, precision, recall, f1)
                            }
                            computed_score <<- scores
                            return(list(score = scores,  failures = failures))
                      }
                  )
       )
model = naiveBayes()
model$fit(X=bow, y=author_labels)
result <- model$score(X_test=X_test, y_test=y_test)
```
## Score of a model (in metrics):
```{r}
model$computed_score
```
### Here are a few sentences that failed to classify correctly:
```{r}
for (i in 1:10) {
  print(result$failures[[i]])
}

```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.

    When evaluating the model, it's important to understand the
    different types of classification results:

    -   A ***true positive*** result is one where the model correctly
        predicts the positive class.
    -   A ***true negative*** result is one where the model correctly
        predicts the negative class.
    -   A ***false positive*** result is one where the model incorrectly
        predicts the positive class when it is actually negative.
    -   A ***false negative*** result is one where the model incorrectly
        predicts the negative class when it is actually positive.

    Precision measures the proportion of true positive predictions among
    all positive predictions made by the model.

    $$
    Precision = \frac{TP}{TP+FP}
    $$

    Recall, on the other hand, measures the proportion of true positives
    identified out of all actual positive cases.

    $$
    Recall = \frac{TP}{TP+FN}
    $$

    F1 score is the harmonic mean of both precision and recall.

    $$
    F1 = \frac{2\times Precision \times Recall}{Precision + Recall}
    $$

    **See [this
    link](https://cohere.com/blog/classification-eval-metrics) to find
    more information about metrics.**

-   Visualize them.

-   Show failure cases.

## Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.
-   Explain why accuracy is not a good choice for the base metrics for
    classification tasks. Why F1 score is always preferable?
