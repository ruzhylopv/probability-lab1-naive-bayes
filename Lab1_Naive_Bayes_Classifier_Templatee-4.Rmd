# nolint: quotes_linter.
---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work breakdown

-   *Name1 Surname1*:
-   *Name2 Surname2*:
-   *Name3 Surname3*:

## Introduction

During the first three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the **Bayes
theorem**.

**Naive Bayes Classifier** is a simple algorithm, which is based on
**Bayes theorem** and used for solving classification problems.
**Classification problem** is a problem in which an observation has to
be classified in one of the $n$ classes based on its similarity with
observations in each class.

It is a **probabilistic classifier**, which means it predicts based on
the probability of an observation belonging to each class. To compute
it, this algorithm uses **Bayes' formula,** that you probably already
came across in **Lesson 3:**
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong **independence** assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given observation
(*For example, if an observation is presented as a sentence, then each
word can be a feature*). Thus,
$\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now can be calculated
as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated as corresponding
relative frequencies using available data\

**See [*this
link*](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)
for more detailed explanations & examples :) Also you can watch [*this
video*](https://youtu.be/O2L2Uv9pdDA?si=-ohkHVDuu3sLLGMq) for more
examples!**

## Data description

There are 5 datasets uploaded on the cms (data.zip)

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one is used to find the probabilities of the corresponding classes
and the second one is used to test your classifier afterwards. Note that
it is crucial to randomly split your data into training and testing
parts to test the classifierʼs possibilities on the unseen data.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(SnowballC)

# Optional libraries - only load if available
if (require(wordcloud, quietly = TRUE)) {
  library(wordcloud)
} else {
  cat("wordcloud not available - skipping word clouds\n")
}

if (require(RColorBrewer, quietly = TRUE)) {
  library(RColorBrewer)
} else {
  cat("RColorBrewer not available - using default colors\n")
}

if (require(caret, quietly = TRUE)) {
  library(caret)
} else {
  cat("caret not available - using basic evaluation\n")
}

if (require(pROC, quietly = TRUE)) {
  library(pROC)
} else {
  cat("pROC not available - skipping ROC curves\n")
}
```

## Outline of the work

1.  **Data pre-processing** (includes removing punctuation marks and
    stop words, representing each message as a bag-of-words)
2.  **Data visualization** (it's time to plot your data!)
3.  **Classifier implementation** (using the training set, calculate all
    the conditional probabilities in formula (1))
4.  **Measurements of effectiveness of your classifier** (use the
    results from the previous step to predict classes for messages in
    the testing set and measure the accuracy, precision and recall, F1
    score metric etc)
5.  **Conclusions**

*!! do not forget to submit both the (compiled) Rmd source file and the
.html output !!*

## Data pre-processing

-   Read the *.csv* data files.
-   Сlear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("data/0-authors")
```

```{r}
test_path <- "data/0-authors/test.csv"
train_path <- "data/0-authors/train.csv"

# Read stop words properly
stop_words <- readLines("stop_words.txt")
# Remove empty lines and trim whitespace
splitted_stop_words <<- trimws(stop_words[stop_words != ""])
cat("Loaded", length(splitted_stop_words), "stop words\n")
cat("First 10 stop words:", head(splitted_stop_words, 10), "\n")
```
```{r}
train <-  read.csv(file = train_path, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
# Data preprocessing: tokenization and stop word removal
cat("Processing training data...\n")
cat("Before filtering - total words:", nrow(unnest_tokens(train, "splitted", 'text', token="words")), "\n")

tidy_text <- unnest_tokens(train, "splitted", 'text', token="words") %>%
    filter(!splitted %in% splitted_stop_words) %>%
    filter(nchar(splitted) > 2)  # Remove very short words

cat("After filtering - remaining words:", nrow(tidy_text), "\n")
cat("Stop words removed:", nrow(unnest_tokens(train, "splitted", 'text', token="words")) - nrow(tidy_text), "\n")

# Apply stemming to reduce vocabulary size
tidy_text <- tidy_text %>% 
    mutate(stemmed = wordStem(splitted, language="en"))

len_of_vocab_before_stemming <- length(unique(tidy_text$splitted))
len_of_vocab_after_stemming <- length(unique(tidy_text$stemmed))

cat("Vocabulary before stemming:", len_of_vocab_before_stemming, "\n")
cat("Vocabulary after stemming:", len_of_vocab_after_stemming, "\n")
cat("Vocabulary reduction:", len_of_vocab_before_stemming - len_of_vocab_after_stemming, "\n")

# Use stemmed words for further processing
tidy_text$splitted <- tidy_text$stemmed
tidy_text <- tidy_text %>% select(-stemmed)

head(tidy_text)
```
The vocabulary without using the stemming algorithm (See https://machinelearningmastery.com/gentle-introduction-bag-words-model/) \\
was 22211 words. Now we reduced it to 13479 using SnowballC library stemming algorithm
```{r}
# Converting each message into bag of words:
vocabulary <- unique(tidy_text$splitted)
authors <- unique(tidy_text$author)

# Create bag of words representation
bow <- tidy_text %>% 
  count(id, splitted) %>%
  pivot_wider(
    names_from = splitted,
    values_from = n,
    values_fill = list(n = 0),
    names_repair = "unique"
  )

# Get author labels for each document
author_labels <- tidy_text %>%
  select(id, author) %>%
  distinct() %>%
  arrange(id)

# Ensure bow and author_labels are aligned
bow <- bow %>% arrange(id)
author_labels <- author_labels %>% arrange(id)

cat("Bag of words dimensions:", nrow(bow), "x", ncol(bow), "\n")
cat("Number of unique words:", length(vocabulary), "\n")
cat("Number of authors:", length(authors), "\n")
cat("Authors:", paste(authors, collapse = ", "), "\n")

head(bow[, 1:10])  # Show first 10 columns
```

## Data visualization

```{r}
# Word frequency analysis by author
word_freq <- tidy_text %>%
  group_by(author, splitted) %>%
  summarise(freq = n(), .groups = "drop") %>%
  arrange(author, desc(freq))

# Overall word frequency (limit for faster processing)
overall_word_freq <- tidy_text %>%
  count(splitted, sort = TRUE) %>%
  head(15)  # Reduced from 20 to 15

# Plot 1: Most common words overall
ggplot(overall_word_freq, aes(x = reorder(splitted, n), y = n)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  coord_flip() +
  labs(title = "Top 20 Most Common Words in Training Data",
       x = "Words", y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))
```

```{r}
# Plot 2: Word frequency by author
top_words_per_author <- word_freq %>%
  group_by(author) %>%
  slice_head(n = 8) %>%  # Reduced from 10 to 8
  ungroup()

ggplot(top_words_per_author, aes(x = reorder(splitted, freq), y = freq, fill = author)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~author, scales = "free_y") +
  labs(title = "Top 10 Words by Author",
       x = "Words", y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "none")
```

```{r}
# Plot 3: Word clouds for each author (only if wordcloud is available)
if (require(wordcloud, quietly = TRUE) && require(RColorBrewer, quietly = TRUE)) {
  par(mfrow = c(1, 3))
  for (author_name in authors) {
    author_words <- word_freq %>%
      filter(author == author_name) %>%
      head(30)  # Reduced from 50 to 30 for faster rendering
    
    wordcloud(words = author_words$splitted, 
              freq = author_words$freq,
              max.words = 30,
              random.order = FALSE,
              colors = brewer.pal(8, "Dark2"),
              main = paste("Word Cloud:", author_name))
  }
  par(mfrow = c(1, 1))
} else {
  cat("Word clouds skipped - wordcloud or RColorBrewer not available\n")
}
```

```{r}
# Plot 4: Document length distribution by author
doc_lengths <- tidy_text %>%
  group_by(id, author) %>%
  summarise(word_count = n(), .groups = "drop")

ggplot(doc_lengths, aes(x = author, y = word_count, fill = author)) +
  geom_boxplot(alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.5) +
  labs(title = "Document Length Distribution by Author",
       x = "Author", y = "Word Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "none") +
  scale_y_log10()
```

```{r}
# Plot 5: Class distribution
class_dist <- table(author_labels$author)
class_dist_df <- data.frame(
  Author = names(class_dist),
  Count = as.numeric(class_dist),
  Percentage = round(as.numeric(class_dist) / sum(class_dist) * 100, 1)
)

ggplot(class_dist_df, aes(x = Author, y = Count, fill = Author)) +
  geom_col(alpha = 0.8) +
  geom_text(aes(label = paste0(Count, "\n(", Percentage, "%)")), 
            vjust = -0.5, size = 4) +
  labs(title = "Class Distribution in Training Data",
       x = "Author", y = "Number of Documents") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "none")
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
                          
       # here it would be wise to have some vars to store intermediate result
       # frequency dict etc. Though pay attention to bag of words! 
       fields = list(
         class_priors = "table",
         #Word probabilities -> probabilities of words given class.
         #Structure:
         #HP Lovecraft
         #hello   free  money    now 
        #  0.25   0.25   0.40   0.10
         word_probabilities = "list",
        
        #A vector of all words available in the training data
         vocabulary = "character",
        
        #A number of all words in training data used to find P(feature)
        total_words_in_X = "numeric"
       ),
       methods = list(
                    # prepare your training data as X - bag of words for each of your
                    # messages and corresponding label for the message encoded as 0 or 1 
                    # (binary classification task)
                    fit = function(X, y)
                    {
                      # Fix column name issue - use the correct column name
                      id_col <- colnames(X)[1]  # First column should be the ID
                      X <- X[order(X[[id_col]]), ]
                      y <- y[order(y$id), ]
                      
                      stopifnot(all(X[[id_col]] == y$id))
                      
                      vocabulary <<- setdiff(colnames(X), id_col)
                      class_counts <- table(y$author)
                      total_words_in_X <<- sum(colSums(X[, 2:ncol(X)]))

   
                      class_priors <<- class_counts / sum(class_counts)
                      word_probabilities <<- list()
                        for (class in names(class_counts)) {
                          # subset X where y == class
                          X_class <- X[y$author == class, , drop = FALSE]
                          # sum counts of each word in that class
                          word_counts <- colSums(X_class[, 2:ncol(X_class)])
                          
                          # add +1 to each count to avoid multiplying by 0
                          word_counts <- word_counts + 1
                          total_count <- sum(word_counts)
                          

                          probs <- word_counts / total_count
                          
                          word_probabilities[[class]] <<- probs
                    }
                      
                      
                         
                    },
                    
                    # return prediction for a single message 
                      predict = function(message) {
                        message_df <- data.frame(text = message, stringsAsFactors = FALSE)
                        tidy_message <- message_df %>%
                          unnest_tokens(splitted, text, token = "words") %>%
                          filter(!(splitted %in% splitted_stop_words)) %>%
                          filter(nchar(splitted) > 2) %>%
                          mutate(stemmed = wordStem(splitted, language="en")) %>%
                          filter(stemmed %in% vocabulary)
                        
                        if (nrow(tidy_message) == 0) {
                          # If no words found, return most common class
                          return(names(which.max(class_priors)))
                        }
                        
                        probs_per_class <- c()
                        
                        for (class in names(class_priors)) {
                          p_class <- class_priors[class]
                          p_observation_given_class <- 1
                          
                          for (word in tidy_message$stemmed) {
                            if (word %in% names(word_probabilities[[class]])) {
                              p_w_given_class <- word_probabilities[[class]][word]
                            } else {
                              # Laplace smoothing for unseen words
                              p_w_given_class <- 1 / (total_words_in_X + length(vocabulary))
                            }
                            p_observation_given_class <- p_observation_given_class * p_w_given_class
                          }
                          
                          probs_per_class[class] <- (p_class * p_observation_given_class)
                        }
                        
                        predicted_class <- names(which.max(probs_per_class))
                        return(predicted_class)
                    }
                  ,
                                      
                                      # score you test set so to get the understanding how well you model
                                      # works.
                                      # look at f1 score or precision and recall
                                      # visualize them 
                                      # try how well your model generalizes to real world data! 
                      score = function(X_test, y_test)
                      {
                        predictions <- c()
                        probabilities <- matrix(0, nrow = nrow(X_test), ncol = length(class_priors))
                        colnames(probabilities) <- names(class_priors)
                        
                        for (i in 1:nrow(X_test)) {
                          pred <- predict(X_test$text[i])
                          predictions[i] <- pred
                          
                          # Calculate probabilities for each class
                          message_df <- data.frame(text = X_test$text[i], stringsAsFactors = FALSE)
                          tidy_message <- message_df %>%
                            unnest_tokens(splitted, text, token = "words") %>%
                            filter(!(splitted %in% splitted_stop_words)) %>%
                            filter(nchar(splitted) > 2) %>%
                            mutate(stemmed = wordStem(splitted, language="en")) %>%
                            filter(stemmed %in% vocabulary)
                          
                          if (nrow(tidy_message) == 0) {
                            probabilities[i, ] <- as.numeric(class_priors)
                          } else {
                            for (j in 1:length(class_priors)) {
                              class_name <- names(class_priors)[j]
                              p_class <- class_priors[class_name]
                              p_observation_given_class <- 1
                              
                              for (word in tidy_message$stemmed) {
                                if (word %in% names(word_probabilities[[class_name]])) {
                                  p_w_given_class <- word_probabilities[[class_name]][word]
                                } else {
                                  p_w_given_class <- 1 / (total_words_in_X + length(vocabulary))
                                }
                                p_observation_given_class <- p_observation_given_class * p_w_given_class
                              }
                              probabilities[i, j] <- p_class * p_observation_given_class
                            }
                          }
                        }
                        
                        # Calculate metrics
                        actual <- y_test$author
                        predicted <- factor(predictions, levels = levels(factor(actual)))
                        actual <- factor(actual)
                        
                        # Confusion matrix - use basic if caret not available
                        if (require(caret, quietly = TRUE)) {
                          cm <- confusionMatrix(predicted, actual)
                          
                          # Calculate F1 scores for each class
                          f1_scores <- c()
                          for (class in levels(actual)) {
                            precision <- cm$byClass[class, "Precision"]
                            recall <- cm$byClass[class, "Recall"]
                            f1_scores[class] <- 2 * (precision * recall) / (precision + recall)
                          }
                        } else {
                          # Basic confusion matrix calculation
                          cm_table <- table(predicted, actual)
                          cm <- list(table = cm_table)
                          
                          # Basic F1 calculation
                          f1_scores <- c()
                          for (class in levels(actual)) {
                            tp <- sum(predicted == class & actual == class)
                            fp <- sum(predicted == class & actual != class)
                            fn <- sum(predicted != class & actual == class)
                            
                            precision <- tp / (tp + fp)
                            recall <- tp / (tp + fn)
                            f1_scores[class] <- 2 * (precision * recall) / (precision + recall)
                          }
                        }
                        
                        # Overall accuracy
                        if (require(caret, quietly = TRUE)) {
                          accuracy <- cm$overall["Accuracy"]
                        } else {
                          accuracy <- sum(predicted == actual) / length(actual)
                        }
                        
                        # Macro-averaged F1
                        macro_f1 <- mean(f1_scores, na.rm = TRUE)
                        
                        return(list(
                          predictions = predictions,
                          probabilities = probabilities,
                          confusion_matrix = cm,
                          accuracy = accuracy,
                          f1_scores = f1_scores,
                          macro_f1 = macro_f1,
                          detailed_metrics = if (require(caret, quietly = TRUE)) cm$byClass else NULL
                        ))
                      }
                  ))

# Train the model
model = naiveBayes()
model$fit(X=bow, y=author_labels)

cat("Model training completed!\n")
cat("Class priors:\n")
print(model$class_priors)
cat("Vocabulary size:", length(model$vocabulary), "\n")
```

```{r}
# Test the model on a few examples
cat("Testing model on sample texts:\n")
sample_texts <- c(
  "The old man sat by the fire, reading ancient books.",
  "In the depths of the ocean, strange creatures lurk.",
  "The detective examined the evidence carefully."
)

for (i in 1:length(sample_texts)) {
  prediction <- model$predict(sample_texts[i])
  cat("Text", i, ":", substr(sample_texts[i], 1, 50), "...\n")
  cat("Predicted author:", prediction, "\n\n")
}
```

## Measure effectiveness of your classifier

```{r}
# Process test data
cat("Processing test data...\n")
test_tidy <- unnest_tokens(test, "splitted", 'text', token="words") %>%
    filter(!splitted %in% splitted_stop_words) %>%
    filter(nchar(splitted) > 2) %>%
    mutate(stemmed = wordStem(splitted, language="en"))

cat("Test data - words before filtering:", nrow(unnest_tokens(test, "splitted", 'text', token="words")), "\n")
cat("Test data - words after filtering:", nrow(test_tidy), "\n")

# Create test bag of words
test_bow <- test_tidy %>% 
  count(id, stemmed) %>%
  pivot_wider(
    names_from = stemmed,
    values_from = n,
    values_fill = list(n = 0),
    names_repair = "unique"
  )

test_author_labels <- test_tidy %>%
  select(id, author) %>%
  distinct() %>%
  arrange(id)

test_bow <- test_bow %>% arrange(id)
test_author_labels <- test_author_labels %>% arrange(id)

cat("Test data processed. Dimensions:", nrow(test_bow), "x", ncol(test_bow), "\n")
```

```{r}
# Evaluate the model (limit test size for faster execution)
cat("Evaluating model on test data...\n")
# Limit test data to first 1000 examples for faster execution
test_subset <- test[1:min(1000, nrow(test)), ]
test_author_labels_subset <- test_author_labels[1:min(1000, nrow(test_author_labels)), ]
results <- model$score(test_subset, test_author_labels_subset)

# Print results
cat("Overall Accuracy:", round(results$accuracy, 4), "\n")
cat("Macro-averaged F1 Score:", round(results$macro_f1, 4), "\n\n")

cat("Per-class F1 Scores:\n")
for (author in names(results$f1_scores)) {
  cat(author, ":", round(results$f1_scores[author], 4), "\n")
}
```

```{r}
# Confusion Matrix Visualization
if (require(caret, quietly = TRUE)) {
  cm_data <- as.data.frame(results$confusion_matrix$table)
  cm_data$Prediction <- factor(cm_data$Prediction, levels = rev(levels(cm_data$Prediction)))
  
  ggplot(cm_data, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(alpha = 0.8) +
    geom_text(aes(label = Freq), color = "white", size = 6, fontface = "bold") +
    scale_fill_gradient(low = "lightblue", high = "darkblue") +
    labs(title = "Confusion Matrix",
         x = "Actual Author", y = "Predicted Author") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
          axis.text.x = element_text(angle = 45, hjust = 1))
} else {
  # Basic confusion matrix plot
  cm_table <- results$confusion_matrix$table
  print("Confusion Matrix:")
  print(cm_table)
}
```

```{r}
# Detailed metrics visualization
if (require(caret, quietly = TRUE) && !is.null(results$detailed_metrics)) {
  metrics_df <- as.data.frame(results$detailed_metrics)
  metrics_df$Class <- rownames(metrics_df)
  metrics_df$Class <- factor(metrics_df$Class, levels = metrics_df$Class)
  
  # Precision, Recall, F1 comparison
  metrics_long <- metrics_df %>%
    select(Class, Precision, Recall, F1) %>%
    pivot_longer(cols = c(Precision, Recall, F1), 
                 names_to = "Metric", values_to = "Value")
  
  ggplot(metrics_long, aes(x = Class, y = Value, fill = Metric)) +
    geom_col(position = "dodge", alpha = 0.8) +
    labs(title = "Precision, Recall, and F1 Scores by Class",
         x = "Author", y = "Score") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
          axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(limits = c(0, 1))
} else {
  # Basic metrics display
  cat("Detailed metrics:\n")
  for (author in names(results$f1_scores)) {
    cat(author, "F1:", round(results$f1_scores[author], 4), "\n")
  }
}
```

```{r}
# ROC Curves for each class (one-vs-rest) - only if pROC is available
if (require(pROC, quietly = TRUE)) {
  # Create binary classification for each author
  roc_data <- data.frame()
  for (author in unique(test_author_labels$author)) {
    binary_actual <- ifelse(test_author_labels$author == author, 1, 0)
    binary_prob <- results$probabilities[, author]
    
    roc_obj <- roc(binary_actual, binary_prob, quiet = TRUE)
    roc_df <- data.frame(
      Author = author,
      Specificity = 1 - roc_obj$specificities,
      Sensitivity = roc_obj$sensitivities
    )
    roc_data <- rbind(roc_data, roc_df)
  }
  
  ggplot(roc_data, aes(x = Specificity, y = Sensitivity, color = Author)) +
    geom_line(size = 1.2) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", alpha = 0.5) +
    labs(title = "ROC Curves by Author",
         x = "1 - Specificity (False Positive Rate)",
         y = "Sensitivity (True Positive Rate)") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))
} else {
  cat("ROC curves skipped - pROC not available\n")
}
```

```{r}
# Show some failure cases
cat("Analyzing failure cases...\n")
predictions <- results$predictions
actual <- test_author_labels$author

# Find misclassified examples
misclassified <- which(predictions != actual)
cat("Number of misclassified examples:", length(misclassified), "\n")
cat("Total test examples:", length(actual), "\n")
cat("Error rate:", round(length(misclassified) / length(actual), 4), "\n\n")

if (length(misclassified) > 0) {
  cat("Sample misclassified examples:\n")
  for (i in head(misclassified, 5)) {
    cat("Example", i, ":\n")
    cat("Text:", substr(test$text[i], 1, 100), "...\n")
    cat("Actual:", actual[i], "| Predicted:", predictions[i], "\n\n")
  }
}
```

## Conclusions

### Method Implementation and Mathematical Foundations

The implemented Naive Bayes classifier is based on **Bayes' theorem** and the **naive independence assumption**. The mathematical foundation relies on the formula:

$$\mathsf{P}(\mathrm{class} \mid \mathrm{observation}) = \mathsf{P}(\mathrm{class}) \times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i \mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}$$

The algorithm works by:
1. **Preprocessing**: Converting text into bag-of-words representation, removing stop words, and applying stemming
2. **Training**: Calculating class priors and word probabilities for each class using maximum likelihood estimation
3. **Prediction**: Computing posterior probabilities for each class and selecting the class with highest probability
4. **Smoothing**: Using Laplace smoothing to handle unseen words in test data

### Pros and Cons of the Method

**Advantages:**
- **Simplicity**: Easy to implement and understand
- **Fast training and prediction**: Computationally efficient
- **Works well with small datasets**: Doesn't require large amounts of training data
- **Handles multiple classes naturally**: No need for one-vs-rest approaches
- **Robust to irrelevant features**: Due to independence assumption
- **Probabilistic output**: Provides confidence scores for predictions

**Disadvantages:**
- **Naive independence assumption**: Words are assumed to be independent, which is rarely true in natural language
- **Feature independence**: Ignores word order and context
- **Zero probability problem**: Without smoothing, unseen words cause zero probabilities
- **Limited expressiveness**: Cannot capture complex relationships between words
- **Sensitive to feature selection**: Poor preprocessing can significantly impact performance

**Limitations:**
- Assumes that all features (words) are conditionally independent given the class
- Treats text as a bag of words, losing important sequential information
- Performance depends heavily on preprocessing quality
- May struggle with domain-specific terminology or very short texts

### Why F1 Score is Preferable to Accuracy

**Accuracy limitations:**
- **Class imbalance**: In imbalanced datasets, accuracy can be misleading (e.g., 95% accuracy when 95% of data belongs to one class)
- **Equal weight to all errors**: Treats false positives and false negatives equally, which may not reflect real-world costs
- **Doesn't show per-class performance**: May hide poor performance on minority classes

**F1 Score advantages:**
- **Balanced metric**: Harmonic mean of precision and recall provides balanced view
- **Handles class imbalance**: More informative when classes are imbalanced
- **Per-class evaluation**: Can be calculated for each class separately
- **Real-world relevance**: Often more meaningful for practical applications where both precision and recall matter

**When to use each:**
- **Accuracy**: When classes are balanced and all misclassifications have equal cost
- **F1 Score**: When classes are imbalanced or when false positives and false negatives have different costs
- **Precision**: When false positives are costly (e.g., spam detection)
- **Recall**: When false negatives are costly (e.g., medical diagnosis)

### Final Performance Summary

The implemented Naive Bayes classifier achieved reasonable performance on the author classification task, demonstrating the effectiveness of this simple yet powerful algorithm for text classification. The comprehensive evaluation using multiple metrics (accuracy, precision, recall, F1-score, and ROC curves) provides a thorough understanding of the model's strengths and weaknesses across different authors and classification scenarios.
